{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/marcellusruben/medium-resources/blob/main/Text_Classification_BERT/bert_medium.ipynb"
      ],
      "metadata": {
        "id": "A1aXb8AfEX68"
      },
      "id": "A1aXb8AfEX68",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QWhuV0zN6jII",
        "outputId": "2638a455-9570-4382-80f5-961ea1a0fc2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QWhuV0zN6jII",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pickle5\n",
        "!pip install toposort"
      ],
      "metadata": {
        "id": "Rl_Rp7JA7ZSA",
        "outputId": "b9fd6134-c4b5-43ac-f342-fc618586b254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Rl_Rp7JA7ZSA",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle5 as pickle"
      ],
      "metadata": {
        "id": "N8ayuz1v6qNz"
      },
      "id": "N8ayuz1v6qNz",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = f'{data_dir}/model'"
      ],
      "metadata": {
        "id": "HpBq7gz_jOIx"
      },
      "id": "HpBq7gz_jOIx",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "challenging-intent",
      "metadata": {
        "id": "challenging-intent"
      },
      "outputs": [],
      "source": [
        "data_dir = 'drive/MyDrive/stat940/dc2'\n",
        "\n",
        "with open(f\"{data_dir}/train.pickle\", \"rb\") as f:\n",
        "    train_dict = pickle.load(f)\n",
        "\n",
        "with open(f\"{data_dir}/test.pickle\", \"rb\") as f:  \n",
        "    test_dict = pickle.load(f)\n",
        "\n",
        "for key, val in test_dict.items():\n",
        "   test_dict[key] = val[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.11.2 "
      ],
      "metadata": {
        "id": "h6khCoe1JcBl",
        "outputId": "7002e20c-138e-4de4-b146-f16c24db4e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h6khCoe1JcBl",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.11.2 in /usr/local/lib/python3.7/dist-packages (4.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (4.63.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (0.0.47)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.2) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.11.2) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11.2) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.2) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "from toposort import toposort\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "hdXh8GCHErYc"
      },
      "id": "hdXh8GCHErYc",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {DEVICE}')"
      ],
      "metadata": {
        "id": "dJLbXrplSvp9",
        "outputId": "2eb37332-38ad-40eb-f65b-3f6297264776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dJLbXrplSvp9",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "\n",
        "        texts, labels = self.tokenize_dict(data_dict)\n",
        "\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "    \n",
        "    def tokenize_dict(self, data_dict):\n",
        "        texts = []\n",
        "        labels = []\n",
        "        t0 = time.time()\n",
        "        for k, (sentences, positions) in data_dict.items():\n",
        "            if (k+1) % 5000 == 0:\n",
        "                print(f'{k} samples processed, split time:{time.time()-t0}')\n",
        "                t0 = time.time()\n",
        "            for i, s in enumerate(sentences):\n",
        "                for j in range(i+1, len(sentences)):\n",
        "                    labels.append(1 if positions[i] < positions[j] else 0)\n",
        "                    texts.append(tokenizer(s+sentences[j], \n",
        "                                           padding='max_length', \n",
        "                                           max_length = 45, \n",
        "                                           truncation=True, \n",
        "                                           return_tensors=\"pt\"))\n",
        "        return texts, labels\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "\n",
        "class Testset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dict):\n",
        "\n",
        "        texts = self.tokenize_dict(data_dict)\n",
        "\n",
        "        self.texts = texts\n",
        "    \n",
        "    def tokenize_dict(self, data_dict):\n",
        "        texts = []\n",
        "        t0 = time.time()\n",
        "        for k, sentences in data_dict.items():\n",
        "            if (k+1) % 5000 == 0:\n",
        "                print(f'{k} samples processed, split time:{time.time()-t0}')\n",
        "                t0 = time.time()\n",
        "            edges = {}\n",
        "            for i, s in enumerate(sentences):\n",
        "                for j in range(i+1, len(sentences)):\n",
        "                    edges[(i,j)] = (tokenizer(s+sentences[j], \n",
        "                                              padding='max_length', \n",
        "                                              max_length = 45, \n",
        "                                              truncation=True, \n",
        "                                              return_tensors=\"pt\"))\n",
        "            texts.append((k, edges))\n",
        "        \n",
        "        return texts\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        key, batch_texts = self.get_batch_texts(idx)\n",
        "\n",
        "        return key, batch_texts"
      ],
      "metadata": {
        "id": "Rw6k1XRkE5pF"
      },
      "id": "Rw6k1XRkE5pF",
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "ay1cI0quFwQf"
      },
      "id": "ay1cI0quFwQf",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "\n",
        "    #train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    #train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    #val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "                \n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "                \n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for val_input, val_label in val_dataloader:\n",
        "\n",
        "                    val_label = val_label.to(device)\n",
        "                    mask = val_input['attention_mask'].to(device)\n",
        "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                    output = model(input_id, mask)\n",
        "\n",
        "                    batch_loss = criterion(output, val_label)\n",
        "                    total_loss_val += batch_loss.item()\n",
        "                    \n",
        "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                    total_acc_val += acc\n",
        "            \n",
        "            print(\n",
        "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataloader): .3f} | Train Accuracy: {total_acc_train / len(train_dataloader): .3f} | Val Loss: {total_loss_val / len(valid_dataloader): .3f} | Val Accuracy: {total_acc_val / len(valid_dataloader): .3f}')\n",
        "                  \n",
        "\n"
      ],
      "metadata": {
        "id": "swvshVwuGfgf"
      },
      "id": "swvshVwuGfgf",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataloader):\n",
        "\n",
        "    #test = Dataset(test_data)\n",
        "\n",
        "    #test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    predictions = {}\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, edge_inputs in test_dataloader:\n",
        "\n",
        "              edge_weights = {}\n",
        "\n",
        "              for (i,j), test_input in edge_inputs.items():\n",
        "                  mask = test_input['attention_mask'].to(device)\n",
        "                  input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                  output = model(input_id, mask)\n",
        "                  edge_weights[(i,j)] = (int(output.argmax(dim=1)), \n",
        "                                         float(torch.max(output)))\n",
        "\n",
        "              predictions[idx] = edge_weights\n",
        "                  #batch_pred = list() \n",
        "                  #predictions += batch_pred\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "Sq456S0rG8Oz"
      },
      "id": "Sq456S0rG8Oz",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = int(0.99*len(train_dict))\n",
        "valid_dict = {i: train_dict[i] for i in range(cutoff, len(train_dict))}\n",
        "train_dict = {i: train_dict[i] for i in range(cutoff)}"
      ],
      "metadata": {
        "id": "1_u4fUzlM_Ks"
      },
      "id": "1_u4fUzlM_Ks",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = Dataset(train_dict)\n",
        "valid_set = Dataset(valid_dict)"
      ],
      "metadata": {
        "id": "P3nxIl2nNzIS",
        "outputId": "44265e74-2830-4155-f035-977a3d4b5c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "P3nxIl2nNzIS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4999 samples processed, split time:6.755975008010864\n",
            "9999 samples processed, split time:7.348271369934082\n",
            "14999 samples processed, split time:7.29736852645874\n",
            "19999 samples processed, split time:7.278870344161987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = Testset(test_dict)"
      ],
      "metadata": {
        "id": "-M9rZsPczt6n"
      },
      "id": "-M9rZsPczt6n",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_set, batch_size=256)"
      ],
      "metadata": {
        "id": "cRMgNNuMTqxo",
        "outputId": "5d70a083-93e2-457c-f84c-a55aba9a8e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "id": "cRMgNNuMTqxo",
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-a6bdef9019f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=1)"
      ],
      "metadata": {
        "id": "Wi4qQDa80jWu"
      },
      "id": "Wi4qQDa80jWu",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = BertClassifier()\n",
        "model = torch.load(model_path)\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "id": "RWzHeWJ_R3Cy",
        "outputId": "44a9e08f-ed43-4e5b-82ea-6059158e1411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RWzHeWJ_R3Cy",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=5, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "LR = 1e-6\n",
        "              \n",
        "train(model, train_dataloader, valid_dataloader, LR, EPOCHS)"
      ],
      "metadata": {
        "id": "vjTKeNvMJ0CA",
        "outputId": "0d3e7107-c10e-473c-8108-a3e6a5e9b0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "id": "vjTKeNvMJ0CA",
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-165-c62fcd0b2600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "total_loss, total_acc = 0, 0\n",
        "with torch.no_grad():\n",
        "\n",
        "    for val_input, val_label in valid_dataloader:\n",
        "\n",
        "        val_label = val_label.to(DEVICE)\n",
        "        mask = val_input['attention_mask'].to(DEVICE)\n",
        "        input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "\n",
        "        batch_loss = criterion(output, val_label)\n",
        "        total_loss += batch_loss.item()\n",
        "        \n",
        "        acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "        print(f'batch accuracy: {acc/len(val_label)}')\n",
        "        total_acc += acc"
      ],
      "metadata": {
        "id": "XD95GrQojtFk",
        "outputId": "6e78ab14-bd4d-4a78-e486-2b12538037d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XD95GrQojtFk",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.81640625\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.7890625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.81640625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.80078125\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.81640625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.8125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.8125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.80078125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.81640625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.76953125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.7890625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.91796875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.796875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.81640625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.91796875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.828125\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.77734375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.8046875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.79296875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.9296875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.83984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.90625\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.80078125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.91796875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.80078125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.8125\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.91015625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.82421875\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.8515625\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.80078125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.85546875\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.9140625\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.89453125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.890625\n",
            "batch accuracy: 0.83203125\n",
            "batch accuracy: 0.80859375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.859375\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.8359375\n",
            "batch accuracy: 0.8203125\n",
            "batch accuracy: 0.88671875\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8828125\n",
            "batch accuracy: 0.87109375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.87890625\n",
            "batch accuracy: 0.8671875\n",
            "batch accuracy: 0.90234375\n",
            "batch accuracy: 0.875\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.86328125\n",
            "batch accuracy: 0.84765625\n",
            "batch accuracy: 0.8984375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.84375\n",
            "batch accuracy: 0.898876404494382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, save_path)"
      ],
      "metadata": {
        "id": "6sqpaD18wnpj"
      },
      "id": "6sqpaD18wnpj",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_inputs = {key: val[0] for key, val in valid_dict.items()}\n",
        "valid_labels = {key: val[1] for key, val in valid_dict.items()}\n",
        "valid_test = Testset(valid_inputs)\n",
        "valid_test_loader = torch.utils.data.DataLoader(valid_test, batch_size=1)"
      ],
      "metadata": {
        "id": "pRKNnD1ElZhN"
      },
      "id": "pRKNnD1ElZhN",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vpred = evaluate(model, valid_test_loader)"
      ],
      "metadata": {
        "id": "hr3PtvYj4LQE"
      },
      "id": "hr3PtvYj4LQE",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dags = pred2dag(vpred)"
      ],
      "metadata": {
        "id": "UUNWnI-Wqsu2"
      },
      "id": "UUNWnI-Wqsu2",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toposorted_dags = dag2top(dags)"
      ],
      "metadata": {
        "id": "JU1BxvFAqxjm"
      },
      "id": "JU1BxvFAqxjm",
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_arrays = {key: np.array([item for td in tp_dag for item in td])[::-1] for key, tp_dag in toposorted_dags.items()}\n"
      ],
      "metadata": {
        "id": "O76fxj5JSnc8"
      },
      "id": "O76fxj5JSnc8",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred2dag(pred):\n",
        "    dags = {}\n",
        "    for key, p in pred.items():\n",
        "        dag = {}\n",
        "        for (i,j), (in_order, _) in p.items():\n",
        "            if in_order == 1:\n",
        "                if i in dag:\n",
        "                    dag[i].add(j)\n",
        "                else: \n",
        "                    dag[i] = {j}\n",
        "            else:\n",
        "                if j in dag:\n",
        "                    dag[j].add(i)\n",
        "                else: \n",
        "                    dag[j] = {i}\n",
        "        dags[int(key)] = dag\n",
        "    return dags\n",
        "\n",
        "def dag2top(dags):\n",
        "    toposorted_dags = {}\n",
        "    for key, d in dags.items():\n",
        "        try: \n",
        "            top = list(toposort(d))\n",
        "            toposorted_dags[key] = top\n",
        "        except:\n",
        "            continue\n",
        "    return toposorted_dags\n",
        "\n",
        "def spearman_corr(x,y):\n",
        "    rank_x, rank_y = ranks(x), ranks(y)\n",
        "    return pearson_corr(rank_x, rank_y)\n",
        "\n",
        "def ranks(arr: np.array) -> np.array:\n",
        "    order = arr.argsort()\n",
        "    ranks = order.argsort()\n",
        "    return ranks\n",
        "\n",
        "def pearson_corr(x: np.array, y: np.array) -> np.array:\n",
        "    cent_x, cent_y = x - np.mean(x), y - np.mean(y)\n",
        "    return cent_x @ cent_y / (cent_x @ cent_x * cent_y @ cent_y)**0.5"
      ],
      "metadata": {
        "id": "N9l8Dq0n9BQX"
      },
      "id": "N9l8Dq0n9BQX",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(toposorted_dags)/len(dags)"
      ],
      "metadata": {
        "id": "cNo9EAOytKyA",
        "outputId": "35d69f92-853c-4e81-f086-fbe5b1c83253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cNo9EAOytKyA",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8955414012738854"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spearman_coeffs = []\n",
        "for key, pred_order in top_arrays.items():\n",
        "    spearman_coeffs.append(stats.spearmanr(pred_order, valid_labels[key]).correlation)"
      ],
      "metadata": {
        "id": "59TEdgXHtZ_H"
      },
      "id": "59TEdgXHtZ_H",
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(spearman_coeffs)"
      ],
      "metadata": {
        "id": "rYGjH8TrumYk",
        "outputId": "2a56a573-56a4-44cf-c101-874d7c477580",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "rYGjH8TrumYk",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1975817923186344"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_arrays[77642]"
      ],
      "metadata": {
        "id": "-is-ivxVwXf4",
        "outputId": "c2056080-37d8-48c7-e3ef-84d9e51a764f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-is-ivxVwXf4",
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 4, 0, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dict[77642]"
      ],
      "metadata": {
        "id": "xz3Nf5iVwvS7",
        "outputId": "949882b1-0cad-4a58-e3bf-b571d5efe78e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xz3Nf5iVwvS7",
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Luckily Ryan found him and put him back.',\n",
              "  'One day Ryan got a pet hamster.',\n",
              "  'One day Snickers got out of his cage.',\n",
              "  'He remembered to lock it next time!',\n",
              "  'He named it Snickers.'],\n",
              " array([3, 0, 2, 4, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZQMOp9X_w0h7"
      },
      "id": "ZQMOp9X_w0h7",
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(spearman_coeffs)"
      ],
      "metadata": {
        "id": "piLV0QKyxTC9",
        "outputId": "e96d3c4b-dc53-4d32-bd74-1161d77e66a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "id": "piLV0QKyxTC9",
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 20.,  72.,  39.,  80.,  40.,  87.,  92.,  41.,  94., 138.]),\n",
              " array([-1. , -0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "x = stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])"
      ],
      "metadata": {
        "id": "J3TJlxjF1aEC"
      },
      "id": "J3TJlxjF1aEC",
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.spearmanr(np.array([2, 1, 0, 3, 4]), np.array([2, 1, 0, 3, 4]))"
      ],
      "metadata": {
        "id": "k2ksYBoa1rlu",
        "outputId": "f1fb093c-6a6f-442e-f9d3-283556974963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k2ksYBoa1rlu",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpearmanrResult(correlation=0.9999999999999999, pvalue=1.4042654220543672e-24)"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p5yq3HsD1zAZ"
      },
      "id": "p5yq3HsD1zAZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "topological-sort.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}